#!/usr/bin/env python3
"""
Process NYC Yellow Taxi data for taxi demand prediction task - V2
å¤„ç†2024å¹´å…¨å¹´æ•°æ®ï¼Œç”ŸæˆæŒ‰å¤©å’ŒæŒ‰å°æ—¶ä¸¤ç§ç²’åº¦çš„èšåˆæ•°æ®

æ•°æ®ç²’åº¦ï¼š
1. æŒ‰å¤©èšåˆï¼šç”¨äºçƒ­åŠ›å›¾å±•ç¤ºï¼ˆæ¯ä¸ªzoneæ¯å¤©çš„æ€»ä¸Šè½¦/ä¸‹è½¦æ•°ï¼‰
2. æŒ‰å°æ—¶èšåˆï¼šç”¨äºè¯¦ç»†åˆ†æå’Œ24å°æ—¶æ›²çº¿å›¾ï¼ˆæ¯ä¸ªzoneæ¯å¤©æ¯å°æ—¶çš„ä¸Šè½¦/ä¸‹è½¦æ•°ï¼‰
"""

import os
import pandas as pd
import geopandas as gpd
from datetime import datetime
import numpy as np
from shapely.geometry import Point, Polygon
import json
from pathlib import Path
import glob

# -----------------------------
# 0) é…ç½®
# -----------------------------
TAXI_DATA_DIR = "yellow_taxi_data"
TAXI_ZONE_LOOKUP = os.path.join(TAXI_DATA_DIR, "taxi_zone_lookup.csv")
TAXI_ZONE_SHAPEFILE = os.path.join(TAXI_DATA_DIR, "taxi_zones", "taxi_zones.shp")

# è¾“å‡ºæ–‡ä»¶
OUT_DIR = "out/taxi_demand"
OUT_PARQUET_DAILY = os.path.join(OUT_DIR, "taxi_demand_daily.parquet")
OUT_PARQUET_HOURLY = os.path.join(OUT_DIR, "taxi_demand_hourly.parquet")
OUT_GEOJSON_ZONES = os.path.join(OUT_DIR, "taxi_zones_manhattan_web.geojson")
OUT_HOURLY_DATA = os.path.join(OUT_DIR, "taxi_hourly_by_zone.json")
OUT_STATS = os.path.join(OUT_DIR, "taxi_demand_stats_2024.json")

os.makedirs(OUT_DIR, exist_ok=True)

FOCUS_BOROUGH = "Manhattan"

print("=" * 70)
print("å‡ºç§Ÿè½¦éœ€æ±‚é¢„æµ‹æ•°æ®å¤„ç† - 2024å…¨å¹´æ•°æ®")
print("=" * 70)

# -----------------------------
# 1) è¯»å–taxi zoneå…ƒæ•°æ®
# -----------------------------
print("\n[1/8] è¯»å–Taxi Zoneå…ƒæ•°æ®...")
zone_lookup = pd.read_csv(TAXI_ZONE_LOOKUP)
manhattan_zones = zone_lookup[zone_lookup['Borough'] == FOCUS_BOROUGH].copy()
manhattan_zone_ids = set(manhattan_zones['LocationID'].values)
print(f"  æ›¼å“ˆé¡¿åŒºåŸŸ: {len(manhattan_zone_ids)} ä¸ªzones")

zones_gdf = gpd.read_file(TAXI_ZONE_SHAPEFILE)
zones_gdf = zones_gdf.to_crs('EPSG:4326')
zones_gdf = zones_gdf.merge(zone_lookup, left_on='LocationID', right_on='LocationID', how='left')
manhattan_zones_gdf = zones_gdf[zones_gdf['Borough'] == FOCUS_BOROUGH].copy()
print(f"  æ›¼å“ˆé¡¿zones shapefile: {len(manhattan_zones_gdf)} ä¸ªå¤šè¾¹å½¢")

# -----------------------------
# 2) è¯»å–å…¨å¹´å‡ºç§Ÿè½¦æ•°æ®
# -----------------------------
print("\n[2/8] è¯»å–2024å¹´å…¨å¹´å‡ºç§Ÿè½¦æ•°æ®...")

# æŸ¥æ‰¾æ‰€æœ‰2024å¹´çš„parquetæ–‡ä»¶
parquet_files = sorted(glob.glob(os.path.join(TAXI_DATA_DIR, "yellow_tripdata_2024-*.parquet")))
print(f"  æ‰¾åˆ° {len(parquet_files)} ä¸ªæœˆä»½çš„æ•°æ®æ–‡ä»¶")

all_data = []
for pf in parquet_files:
    month_name = os.path.basename(pf)
    print(f"  è¯»å–: {month_name}")
    df_month = pd.read_parquet(pf)
    
    # ç­›é€‰æ›¼å“ˆé¡¿ç›¸å…³è¡Œç¨‹
    df_month = df_month[
        (df_month['PULocationID'].isin(manhattan_zone_ids)) | 
        (df_month['DOLocationID'].isin(manhattan_zone_ids))
    ].copy()
    
    # ç§»é™¤æ— æ•ˆæ•°æ®
    df_month = df_month.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID'])
    
    # åªä¿ç•™éœ€è¦çš„åˆ—
    keep_columns = [
        'tpep_pickup_datetime', 'tpep_dropoff_datetime',
        'PULocationID', 'DOLocationID',
        'passenger_count', 'trip_distance', 'fare_amount'
    ]
    df_month = df_month[keep_columns].copy()
    
    all_data.append(df_month)
    print(f"    â†’ {len(df_month):,} æ¡æ›¼å“ˆé¡¿ç›¸å…³è¡Œç¨‹")

# åˆå¹¶æ‰€æœ‰æœˆä»½æ•°æ®
df = pd.concat(all_data, ignore_index=True)
print(f"\n  å…¨å¹´æ€»æ•°æ®: {len(df):,} æ¡è¡Œç¨‹")

# -----------------------------
# 3) æ—¶é—´ç‰¹å¾æå–
# -----------------------------
print("\n[3/8] æå–æ—¶é—´ç‰¹å¾...")

df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
df['pickup_date'] = df['pickup_datetime'].dt.date
df['pickup_hour'] = df['pickup_datetime'].dt.hour
df['pickup_month'] = df['pickup_datetime'].dt.month

# è¿‡æ»¤2024å¹´æ•°æ®
df = df[(df['pickup_datetime'].dt.year == 2024)].copy()
print(f"  2024å¹´æ•°æ®: {len(df):,} æ¡")
print(f"  æ—¶é—´èŒƒå›´: {df['pickup_datetime'].min()} è‡³ {df['pickup_datetime'].max()}")

# -----------------------------
# 4) æŒ‰å¤©èšåˆéœ€æ±‚æ•°æ®ï¼ˆç”¨äºçƒ­åŠ›å›¾ï¼‰
# -----------------------------
print("\n[4/8] èšåˆæŒ‰å¤©éœ€æ±‚æ•°æ®...")

# ä¸Šè½¦éœ€æ±‚ï¼ˆæŒ‰å¤©ï¼‰
pickup_daily = df.groupby(['pickup_date', 'PULocationID']).agg({
    'tpep_pickup_datetime': 'count',
    'passenger_count': 'sum',
    'trip_distance': 'mean',
    'fare_amount': 'mean'
}).reset_index()
pickup_daily.columns = ['date', 'zone_id', 'pickup_count', 'passenger_sum', 'avg_distance', 'avg_fare']

# ä¸‹è½¦éœ€æ±‚ï¼ˆæŒ‰å¤©ï¼‰
dropoff_daily = df.groupby(['pickup_date', 'DOLocationID']).agg({
    'tpep_dropoff_datetime': 'count'
}).reset_index()
dropoff_daily.columns = ['date', 'zone_id', 'dropoff_count']

# åˆå¹¶
daily_demand = pickup_daily.merge(dropoff_daily, on=['date', 'zone_id'], how='outer')
daily_demand = daily_demand.fillna(0)
daily_demand = daily_demand[daily_demand['zone_id'].isin(manhattan_zone_ids)].copy()

daily_demand['pickup_count'] = daily_demand['pickup_count'].astype(int)
daily_demand['dropoff_count'] = daily_demand['dropoff_count'].astype(int)
daily_demand['passenger_sum'] = daily_demand['passenger_sum'].astype(int)
daily_demand['total_demand'] = daily_demand['pickup_count'] + daily_demand['dropoff_count']

# æ·»åŠ æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ–¹ä¾¿å‰ç«¯æŸ¥è¯¢ï¼‰
daily_demand['date_str'] = daily_demand['date'].astype(str)

print(f"  æŒ‰å¤©èšåˆæ•°æ®: {len(daily_demand):,} æ¡è®°å½•")
print(f"  æ¶‰åŠæ—¥æœŸ: {daily_demand['date'].nunique()} å¤©")
print(f"  æ¶‰åŠzones: {daily_demand['zone_id'].nunique()} ä¸ª")

# -----------------------------
# 5) æŒ‰å°æ—¶èšåˆéœ€æ±‚æ•°æ®ï¼ˆç”¨äº24å°æ—¶æ›²çº¿å›¾ï¼‰
# -----------------------------
print("\n[5/8] èšåˆæŒ‰å°æ—¶éœ€æ±‚æ•°æ®...")

# ä¸Šè½¦éœ€æ±‚ï¼ˆæŒ‰å¤©+å°æ—¶ï¼‰
pickup_hourly = df.groupby(['pickup_date', 'pickup_hour', 'PULocationID']).agg({
    'tpep_pickup_datetime': 'count',
    'passenger_count': 'sum'
}).reset_index()
pickup_hourly.columns = ['date', 'hour', 'zone_id', 'pickup_count', 'passenger_sum']

# ä¸‹è½¦éœ€æ±‚ï¼ˆæŒ‰å¤©+å°æ—¶ï¼‰
dropoff_hourly = df.groupby(['pickup_date', 'pickup_hour', 'DOLocationID']).agg({
    'tpep_dropoff_datetime': 'count'
}).reset_index()
dropoff_hourly.columns = ['date', 'hour', 'zone_id', 'dropoff_count']

# åˆå¹¶
hourly_demand = pickup_hourly.merge(dropoff_hourly, on=['date', 'hour', 'zone_id'], how='outer')
hourly_demand = hourly_demand.fillna(0)
hourly_demand = hourly_demand[hourly_demand['zone_id'].isin(manhattan_zone_ids)].copy()

hourly_demand['pickup_count'] = hourly_demand['pickup_count'].astype(int)
hourly_demand['dropoff_count'] = hourly_demand['dropoff_count'].astype(int)
hourly_demand['passenger_sum'] = hourly_demand['passenger_sum'].astype(int)
hourly_demand['date_str'] = hourly_demand['date'].astype(str)

print(f"  æŒ‰å°æ—¶èšåˆæ•°æ®: {len(hourly_demand):,} æ¡è®°å½•")

# -----------------------------
# 6) ä¿å­˜æ¨¡å‹æ•°æ®ï¼ˆParquetï¼‰
# -----------------------------
print("\n[6/8] ä¿å­˜æ¨¡å‹åˆ†ææ•°æ®...")

# ä¿å­˜æŒ‰å¤©æ•°æ®
daily_demand_with_info = daily_demand.merge(
    manhattan_zones[['LocationID', 'Zone', 'service_zone']], 
    left_on='zone_id', 
    right_on='LocationID',
    how='left'
)
daily_demand_with_info.to_parquet(OUT_PARQUET_DAILY, index=False)
print(f"  âœ“ {OUT_PARQUET_DAILY}")

# ä¿å­˜æŒ‰å°æ—¶æ•°æ®
hourly_demand_with_info = hourly_demand.merge(
    manhattan_zones[['LocationID', 'Zone', 'service_zone']], 
    left_on='zone_id', 
    right_on='LocationID',
    how='left'
)
hourly_demand_with_info.to_parquet(OUT_PARQUET_HOURLY, index=False)
print(f"  âœ“ {OUT_PARQUET_HOURLY}")

# -----------------------------
# 7) ç”ŸæˆWebå¯è§†åŒ–æ•°æ®
# -----------------------------
print("\n[7/8] ç”ŸæˆWebå¯è§†åŒ–æ•°æ®...")

# 7.1) ç”Ÿæˆtaxi zones GeoJSONï¼ˆå¸¦å…¨å¹´æ€»éœ€æ±‚ç»Ÿè®¡ï¼‰
print("  [7.1] ç”Ÿæˆtaxi zoneså¤šè¾¹å½¢...")

zone_total_demand = daily_demand.groupby('zone_id').agg({
    'pickup_count': 'sum',
    'dropoff_count': 'sum',
    'total_demand': 'sum'
}).reset_index()

manhattan_zones_gdf_web = manhattan_zones_gdf.merge(
    zone_total_demand,
    left_on='LocationID',
    right_on='zone_id',
    how='left'
)

manhattan_zones_gdf_web['pickup_count'] = manhattan_zones_gdf_web['pickup_count'].fillna(0).astype(int)
manhattan_zones_gdf_web['dropoff_count'] = manhattan_zones_gdf_web['dropoff_count'].fillna(0).astype(int)
manhattan_zones_gdf_web['total_demand'] = manhattan_zones_gdf_web['total_demand'].fillna(0).astype(int)

# è®¡ç®—ä¸­å¿ƒç‚¹
import warnings
warnings.filterwarnings('ignore')
manhattan_zones_gdf_web['centroid_lon'] = manhattan_zones_gdf_web.geometry.centroid.x
manhattan_zones_gdf_web['centroid_lat'] = manhattan_zones_gdf_web.geometry.centroid.y

zones_web = manhattan_zones_gdf_web[[
    'LocationID', 'Zone', 'Borough', 'service_zone',
    'pickup_count', 'dropoff_count', 'total_demand',
    'centroid_lon', 'centroid_lat', 'geometry'
]].copy()

zones_web.to_file(OUT_GEOJSON_ZONES, driver='GeoJSON')
print(f"  âœ“ {OUT_GEOJSON_ZONES}")

# 7.2) ç”ŸæˆæŒ‰å°æ—¶æ•°æ®çš„JSONï¼ˆä¾›å‰ç«¯æŸ¥è¯¢ï¼‰
print("  [7.2] ç”Ÿæˆhourlyæ•°æ®JSON...")

# åˆ›å»ºåµŒå¥—çš„æ•°æ®ç»“æ„: {zone_id: {date: [24å°æ—¶çš„æ•°æ®]}}
hourly_by_zone = {}

for zone_id in manhattan_zone_ids:
    zone_data = hourly_demand[hourly_demand['zone_id'] == zone_id]
    hourly_by_zone[int(zone_id)] = {}
    
    for date in zone_data['date'].unique():
        date_str = str(date)
        date_data = zone_data[zone_data['date'] == date].sort_values('hour')
        
        # ç¡®ä¿æœ‰å®Œæ•´çš„24å°æ—¶æ•°æ®
        hourly_array = []
        for h in range(24):
            hour_row = date_data[date_data['hour'] == h]
            if len(hour_row) > 0:
                hourly_array.append({
                    'hour': h,
                    'pickup': int(hour_row['pickup_count'].iloc[0]),
                    'dropoff': int(hour_row['dropoff_count'].iloc[0]),
                    'passengers': int(hour_row['passenger_sum'].iloc[0])
                })
            else:
                hourly_array.append({
                    'hour': h,
                    'pickup': 0,
                    'dropoff': 0,
                    'passengers': 0
                })
        
        hourly_by_zone[int(zone_id)][date_str] = hourly_array

with open(OUT_HOURLY_DATA, 'w', encoding='utf-8') as f:
    json.dump(hourly_by_zone, f, separators=(',', ':'))
print(f"  âœ“ {OUT_HOURLY_DATA}")

# -----------------------------
# 8) ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
# -----------------------------
print("\n[8/8] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

stats = {
    "summary": {
        "total_trips": int(len(df)),
        "total_passengers": int(df['passenger_count'].sum()),
        "avg_trip_distance_miles": round(float(df['trip_distance'].mean()), 2),
        "avg_fare_amount": round(float(df['fare_amount'].mean()), 2),
        "time_range": {
            "start": str(df['pickup_datetime'].min()),
            "end": str(df['pickup_datetime'].max())
        },
        "zones_count": len(manhattan_zone_ids),
        "total_days": int(daily_demand['date'].nunique()),
        "daily_data_points": len(daily_demand),
        "hourly_data_points": len(hourly_demand)
    },
    "monthly_stats": [],
    "top_zones": zone_total_demand.merge(
        manhattan_zones[['LocationID', 'Zone']], 
        left_on='zone_id', 
        right_on='LocationID'
    ).sort_values('total_demand', ascending=False).head(10)[[
        'zone_id', 'Zone', 'pickup_count', 'dropoff_count', 'total_demand'
    ]].to_dict('records'),
    "date_range": {
        "min": str(daily_demand['date'].min()),
        "max": str(daily_demand['date'].max())
    }
}

# æŒ‰æœˆç»Ÿè®¡
for month in range(1, 13):
    month_data = df[df['pickup_month'] == month]
    if len(month_data) > 0:
        stats['monthly_stats'].append({
            'month': month,
            'trips': int(len(month_data)),
            'avg_daily_trips': int(len(month_data) / 30)
        })

with open(OUT_STATS, 'w', encoding='utf-8') as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)
print(f"  âœ“ {OUT_STATS}")

# -----------------------------
# 9) æ‰“å°æ‘˜è¦
# -----------------------------
print("\n" + "=" * 70)
print("âœ… å¤„ç†å®Œæˆï¼")
print("=" * 70)
print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
print(f"  â€¢ æ€»è¡Œç¨‹æ•°: {len(df):,}")
print(f"  â€¢ æ€»ä¹˜å®¢æ•°: {int(df['passenger_count'].sum()):,}")
print(f"  â€¢ æ—¶é—´è·¨åº¦: {daily_demand['date'].min()} ~ {daily_demand['date'].max()}")
print(f"  â€¢ è¦†ç›–å¤©æ•°: {daily_demand['date'].nunique()} å¤©")
print(f"  â€¢ æ›¼å“ˆé¡¿zones: {len(manhattan_zone_ids)}")
print(f"  â€¢ æŒ‰å¤©æ•°æ®ç‚¹: {len(daily_demand):,}")
print(f"  â€¢ æŒ‰å°æ—¶æ•°æ®ç‚¹: {len(hourly_demand):,}")

print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
print(f"  â€¢ {OUT_PARQUET_DAILY}")
print(f"  â€¢ {OUT_PARQUET_HOURLY}")
print(f"  â€¢ {OUT_GEOJSON_ZONES}")
print(f"  â€¢ {OUT_HOURLY_DATA}")
print(f"  â€¢ {OUT_STATS}")

print(f"\nğŸ”¥ éœ€æ±‚æœ€é«˜çš„5ä¸ªzonesï¼ˆå…¨å¹´ï¼‰:")
for _, row in zone_total_demand.merge(
    manhattan_zones[['LocationID', 'Zone']], 
    left_on='zone_id', 
    right_on='LocationID'
).sort_values('total_demand', ascending=False).head(5).iterrows():
    print(f"  {row['Zone']}: {int(row['total_demand']):,} (â†‘{int(row['pickup_count']):,} â†“{int(row['dropoff_count']):,})")

print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
print("  1. ä½¿ç”¨ tippecanoe ç”Ÿæˆ mbtiles")
print("  2. æ›´æ–°å‰ç«¯é¡µé¢æ·»åŠ æ—¥æœŸé€‰æ‹©å™¨")
print("  3. å®ç°24å°æ—¶æ›²çº¿å›¾å¼¹çª—")

